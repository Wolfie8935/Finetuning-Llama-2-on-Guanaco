{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Environment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Techniques we will be using:\n",
    "    1. PEFT - Perimeter Effecient Transfer Learning \n",
    "    2. LoRA - Low-Rank Adaptation of LLM\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q accelerate peft bitsandbytes transformers trl\n",
    "# Create a conda environment and run \n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# my python version is 3.12.4 and i will be using GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanc\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "# during transfer learning peft freezes the weights and only some weights will be retrained\n",
    "from peft import LoraConfig, PeftModel \n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will reformat our dataset to follow Llama 2 template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Llama 2 template\\n    \\n    <s>[INST] <<SYS>>\\n    System prompt\\n    <</SYS>>\\n\\n    User prompt [/INST] Model answer </s>\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Llama 2 template\n",
    "    \n",
    "    <s>[INST] <<SYS>>\n",
    "    System prompt\n",
    "    <</SYS>>\n",
    "\n",
    "    User prompt [/INST] Model answer </s>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "dataset_name = 'mlabonne/guanaco-llama2-1k' \n",
    "\n",
    "new_model = 'Llama-2-7b-chat-finetune'\n",
    "\n",
    "## QLoRA parameterrs\n",
    "\n",
    "lora_r = 64                         #LoRA attention dimention\n",
    "lora_alpha = 16                     #LoRA Scaling\n",
    "lora_dropout = 0.1                  #LoRA Dropout\n",
    "\n",
    "## Bites and Bytes parameters\n",
    "\n",
    "use_4bit = True                     #4-bit precision model load\n",
    "bnb_4bit_compute_dtype = 'float16'  #d-type for 4-bit base model\n",
    "bnb_4bit_quant_type = 'nf4'         #fp4 or nf4 - type quant\n",
    "use_nested_quant = False            #nested quantization\n",
    "\n",
    "## Training Arguments parameters\n",
    "\n",
    "output_dir = './results'            #saving model predictions and checkpoints\n",
    "num_train_epochs = 1                #training epochs\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 2     #Batch size for GPU training\n",
    "per_device_eval_batch_size = 2      #Batch size for GPU evaluation\n",
    "gradient_accumulation_steps = 1     #Update steps for gradients\n",
    "gradient_checkpointing = True       #gradient checkpoints\n",
    "max_grad_norm = 0.3                 #gradient clipping\n",
    "learning_rate = 2e-4                #AdamW optimizer\n",
    "weight_decay = 0.001                #Weight decay for all layers\n",
    "optim = 'paged_adamw_32bit'         #Optimizer\n",
    "lr_scheduler_type = 'cosine'        #Learning rate schedule\n",
    "max_steps = -1                      #Number of training steps(overrides epochs)\n",
    "warmup_ratio = 0.03                 #from 0 to linear warmup(learning rate)\n",
    "group_by_length = True              #group sequences wit batches with same length and saves memory and speds up training\n",
    "save_steps = 0                      #saves every X updates steps\n",
    "logging_steps = 25                  #logs every X updates steps\n",
    "\n",
    "## SFT parameters\n",
    "\n",
    "max_seq_length = None               #Max sequence length to use\n",
    "packing = False                     #Pack multiple short examples in the same input to increase effeciency\n",
    "device_map = {\"\": 0}                #Load the entire model on GPU 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load everything and start finetuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\n",
    "\n",
    "\n",
    "2. Then, weâ€™re configuring bitsandbytes for 4-bit quantization.\n",
    "\n",
    "\n",
    "3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\n",
    "\n",
    "\n",
    "4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split='train')\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype) # tokenizer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "## Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "## Load Llama Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'        #wierd issue with fp16\n",
    "\n",
    "## Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "## Set training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "## Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
